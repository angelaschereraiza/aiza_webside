# --- llama.cpp server tuning ---
MODEL_DIR=/opt/aiza-ai/models
MODEL_FILE=qwen2.5-coder-7b-instruct-q4_k_m.gguf
CTX=4096
THREADS=4
BATCH=128

# --- ports ---
LLAMA_PORT=8000
PROXY_PORT=9000